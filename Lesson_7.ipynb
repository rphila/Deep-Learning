{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7: Build a Live Camera App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rita Philavanh\n",
    "\n",
    "20170905"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Street View House Number (SVHN) training/test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download train.tar.gz and test.tar.gz files from http://ufldl.stanford.edu/housenumbers/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset from compressed tar.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already present - Skipping extraction of train.tar.gz.\n",
      "[]\n",
      "test already present - Skipping extraction of test.tar.gz.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 0\n",
    "np.random.seed(133)\n",
    "data_root = '.'\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract('train.tar.gz')\n",
    "test_folders = maybe_extract('test.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract labels from .mat file \n",
    "\n",
    "(used digiStruct.py code found here: https://github.com/prijip/Py-Gsvhn-DigitStruct-Reader/blob/360a03909a22a28e2e814e83d6915ee5f915e5da/digitStruct.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 256.67s\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "# Bounding Box\n",
    "#\n",
    "class BBox:\n",
    "    def __init__(self):\n",
    "        self.label = \"\"     # Digit\n",
    "        self.left = 0\n",
    "        self.top = 0\n",
    "        self.width = 0\n",
    "        self.height = 0\n",
    "\n",
    "class DigitStruct:\n",
    "    def __init__(self):\n",
    "        self.name = None    # Image file name\n",
    "        self.bboxList = None # List of BBox structs\n",
    "\n",
    "# Function for debugging\n",
    "def printHDFObj(theObj, theObjName):\n",
    "    isFile = isinstance(theObj, h5py.File)\n",
    "    isGroup = isinstance(theObj, h5py.Group)\n",
    "    isDataSet = isinstance(theObj, h5py.Dataset)\n",
    "    isReference = isinstance(theObj, h5py.Reference)\n",
    "    print(\"{}\".format(theObjName))\n",
    "    print(\"    type(): {}\".format(type(theObj)))\n",
    "    if isFile or isGroup or isDataSet:\n",
    "        # if theObj.name != None:\n",
    "        #    print \"    name: {}\".format(theObj.name)\n",
    "        print(\"    id: {}\".format(theObj.id))\n",
    "    if isFile or isGroup:\n",
    "        print(\"    keys: {}\".format(theObj.keys()))\n",
    "    if not isReference:\n",
    "        print(\"    Len: {}\".format(len(theObj)))\n",
    "\n",
    "    if not (isFile or isGroup or isDataSet or isReference):\n",
    "        print(theObj)\n",
    "\n",
    "def readDigitStructGroup(dsFile):\n",
    "    dsGroup = dsFile[\"digitStruct\"]\n",
    "    return dsGroup\n",
    "\n",
    "#\n",
    "# Reads a string from the file using its reference\n",
    "#\n",
    "def readString(strRef, dsFile):\n",
    "    strObj = dsFile[strRef]\n",
    "    str = ''.join(chr(i) for i in strObj)\n",
    "    return str\n",
    "\n",
    "#\n",
    "# Reads an integer value from the file\n",
    "#\n",
    "def readInt(intArray, dsFile):\n",
    "    intRef = intArray[0]\n",
    "    isReference = isinstance(intRef, h5py.Reference)\n",
    "    intVal = 0\n",
    "    if isReference:\n",
    "        intObj = dsFile[intRef]\n",
    "        intVal = int(intObj[0])\n",
    "    else: # Assuming value type\n",
    "        intVal = int(intRef)\n",
    "    return intVal\n",
    "\n",
    "def yieldNextInt(intDataset, dsFile):\n",
    "    for intData in intDataset:\n",
    "        intVal = readInt(intData, dsFile)\n",
    "        yield intVal \n",
    "\n",
    "def yieldNextBBox(bboxDataset, dsFile):\n",
    "    for bboxArray in bboxDataset:\n",
    "        bboxGroupRef = bboxArray[0]\n",
    "        bboxGroup = dsFile[bboxGroupRef]\n",
    "        labelDataset = bboxGroup[\"label\"]\n",
    "        leftDataset = bboxGroup[\"left\"]\n",
    "        topDataset = bboxGroup[\"top\"]\n",
    "        widthDataset = bboxGroup[\"width\"]\n",
    "        heightDataset = bboxGroup[\"height\"]\n",
    "\n",
    "        left = yieldNextInt(leftDataset, dsFile)\n",
    "        top = yieldNextInt(topDataset, dsFile)\n",
    "        width = yieldNextInt(widthDataset, dsFile)\n",
    "        height = yieldNextInt(heightDataset, dsFile)\n",
    "\n",
    "        bboxList = []\n",
    "\n",
    "        for label in yieldNextInt(labelDataset, dsFile):\n",
    "            bbox = BBox()\n",
    "            bbox.label = label\n",
    "            bbox.left = next(left)\n",
    "            bbox.top = next(top)\n",
    "            bbox.width = next(width)\n",
    "            bbox.height = next(height)\n",
    "            bboxList.append(bbox)\n",
    "\n",
    "        yield bboxList\n",
    "\n",
    "def yieldNextFileName(nameDataset, dsFile):\n",
    "    for nameArray in nameDataset:\n",
    "        nameRef = nameArray[0]\n",
    "        name = readString(nameRef, dsFile)\n",
    "        yield name\n",
    "\n",
    "# dsFile = h5py.File('train/digitStruct.mat', 'r')\n",
    "def yieldNextDigitStruct(dsFileName):\n",
    "    dsFile = h5py.File(dsFileName, 'r')\n",
    "    dsGroup = readDigitStructGroup(dsFile)\n",
    "    nameDataset = dsGroup[\"name\"]\n",
    "    bboxDataset = dsGroup[\"bbox\"]\n",
    "\n",
    "    bboxListIter = yieldNextBBox(bboxDataset, dsFile)\n",
    "    for name in yieldNextFileName(nameDataset, dsFile):\n",
    "        bboxList = next(bboxListIter)\n",
    "        obj = DigitStruct()\n",
    "        obj.name = name\n",
    "        obj.bboxList = bboxList\n",
    "        yield obj\n",
    "\n",
    "# Declare variables to store labels\n",
    "L = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "y4 = []\n",
    "y5 = []\n",
    "\n",
    "#Declare variables to store overall bounding box\n",
    "left = []\n",
    "top = []\n",
    "width = []\n",
    "height = []\n",
    "\n",
    "def testMain():\n",
    "    i = 0\n",
    "    dsFileName = 'train/digitStruct.mat'\n",
    "    testCounter = 0\n",
    "\n",
    "    for dsObj in yieldNextDigitStruct(dsFileName):\n",
    "        # testCounter += 1\n",
    "       # print(dsObj.name)\n",
    "      #  print(len(dsObj.bboxList))\n",
    "        L.append(len(dsObj.bboxList))\n",
    "        \n",
    "        cnty = 0\n",
    "        width_sum = 0\n",
    "        height_max = 0\n",
    "        for bbox in dsObj.bboxList:\n",
    "         #   print(\"    {}:{},{},{},{}\".format(\n",
    "         #       bbox.label, bbox.left, bbox.top, bbox.width, bbox.height))\n",
    "            \n",
    "            width_sum = width_sum + bbox.width\n",
    "            height_max = max(height_max, bbox.height)\n",
    "                \n",
    "            if cnty == 0: \n",
    "                y1.append(bbox.label)\n",
    "                left.append(bbox.left)\n",
    "                top.append(bbox.top)\n",
    "            elif cnty == 1: y2.append(bbox.label) \n",
    "            elif cnty == 2: y3.append(bbox.label)\n",
    "            elif cnty == 3: y4.append(bbox.label)\n",
    "            elif cnty == 4: y5.append(bbox.label)\n",
    "            \n",
    "            cnty += 1\n",
    "            \n",
    "        width.append(width_sum)\n",
    "        height.append(height_max)\n",
    "\n",
    "        # assign None to empty digits\n",
    "        if cnty < 5:\n",
    "            if len(dsObj.bboxList) < 1: y1.append(None)\n",
    "            if len(dsObj.bboxList) < 2: y2.append(None) \n",
    "            if len(dsObj.bboxList) < 3: y3.append(None)\n",
    "            if len(dsObj.bboxList) < 4: y4.append(None)\n",
    "            if len(dsObj.bboxList) < 5: y5.append(None)\n",
    "        if testCounter >= 5:\n",
    "            break\n",
    "        i +=1\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    t1 = time.time()\n",
    "    testMain()\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"Time: %0.2fs\" % (t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Check Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert\n",
      "33402\n",
      "[2 3 2 3 2 3 2 2 3 3]\n",
      "[4 2 2 2 1 3 2 4 8 1]\n",
      "[2 5 9 0 0 2 4 6 1 0]\n",
      "[10 1 10 4 10 7 10 10 7 3]\n",
      "[10 10 10 10 10 10 10 10 10 10]\n",
      "[10 10 10 10 10 10 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "L = np.asarray(L)\n",
    "y1 = np.asarray(y1)\n",
    "y2 = np.asarray(y2)\n",
    "y3 = np.asarray(y3)\n",
    "y4 = np.asarray(y4)\n",
    "y5 = np.asarray(y5)\n",
    "\n",
    "\n",
    "if len(y3[y3==None]) != 0:\n",
    "  print(\"convert\")\n",
    "# Convert 10 to actual 0 label\n",
    "  y1[y1 == 10] = 0\n",
    "  y2[y2 == 10] = 0\n",
    "  y3[y3 == 10] = 0\n",
    "  y4[y4 == 10] = 0\n",
    "  y5[y5 == 10] = 0\n",
    "# Replace None with 10 label\n",
    "  y1[y1 == None] = 10\n",
    "  y2[y2 == None] = 10\n",
    "  y3[y3 == None] = 10\n",
    "  y4[y4 == None] = 10\n",
    "  y5[y5 == None] = 10\n",
    "    \n",
    "\n",
    "print(len(L))\n",
    "print(L[40:50]) # length of digits\n",
    "print(y1[40:50]) # 1st digit\n",
    "print(y2[40:50]) # 2nd digit\n",
    "print(y3[40:50]) # ...etc (If no digit, then 'None')\n",
    "print(y4[40:50])\n",
    "print(y5[40:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset to cropped 32x32 pixels grayscale matrix fitting outer bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 459.72s\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "X = []\n",
    "pic_num = 1\n",
    "#print(len(os.listdir('./train')))\n",
    "num_files = len([f for f in os.listdir('./train') if f.endswith('.png') and os.path.isfile(os.path.join('./train', f))])\n",
    "for i in range(num_files): \n",
    "    #print(top[i], height[i], left[i], width[i])\n",
    "    img = cv2.imread(\"./train/\"+str(pic_num) +\".png\",cv2.IMREAD_GRAYSCALE)\n",
    "    #print(img.shape)\n",
    "    #print(img[top[i]:(top[i]+height[i]), left[i]:(left[i]+width[i])].shape)\n",
    "    crop_img = img[max(0,top[i]):(max(0,top[i])+max(0,height[i])), max(0,left[i]):(max(0,left[i])+max(0,width[i]))]\n",
    "    resized_image = cv2.resize(crop_img, (32, 32))\n",
    "    X.append(resized_image)\n",
    "   # print(pic_num)\n",
    "    pic_num +=1\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Time: %0.2fs\" % (t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402 13402 20000\n",
      "33402 13402 20000\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:20000]\n",
    "L_train = L[:20000] \n",
    "\n",
    "y1_train = y1[:20000]\n",
    "y2_train = y2[:20000]\n",
    "y3_train = y3[:20000]\n",
    "y4_train = y4[:20000]\n",
    "y5_train = y5[:20000]\n",
    "\n",
    "X_valid = X[20000:]\n",
    "L_valid = L[20000:] \n",
    "y1_valid = y1[20000:]\n",
    "y2_valid = y2[20000:]\n",
    "y3_valid = y3[20000:]\n",
    "y4_valid = y4[20000:]\n",
    "y5_valid = y5[20000:]\n",
    "\n",
    "print(len(X), len(X_valid), len(X_train))\n",
    "print(len(L), len(L_valid), len(L_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 3 3 2]\n",
      "[1 2 2 9 3 3 2 7 1 1]\n",
      "[9 3 5 3 1 3 8 4 2 6]\n",
      "[10 10 10 10 10 10 10 4 8 10]\n",
      "[10 10 10 10 10 10 10 4 8 10]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "[3 1 2 3 3 3 3 2 3 3]\n",
      "[1 7 5 1 2 6 1 4 7 1]\n",
      "[3 10 0 6 6 9 3 1 5 2]\n",
      "[6 10 10 9 7 6 5 10 1 4]\n",
      "[6 10 10 9 7 6 5 10 1 4]\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "Training set (20000, 32, 32, 1) (20000, 6) (20000, 11)\n",
      "Validation set (13402, 32, 32, 1) (13402, 6) (13402, 11)\n"
     ]
    }
   ],
   "source": [
    "image_size = 32 # 32 x 32 pixel\n",
    "num_channels = 1 # grayscale\n",
    "num_labels = 11 # 0-9 + 10 (for missing value)\n",
    "num_digits = 6 #max 5 digit for housenumber\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, label0, label1, label2, label3, label4, label5):\n",
    "  dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  print((label0[:10]))\n",
    "  print((label1[:10]))\n",
    "  print((label2[:10]))\n",
    "  print((label3[:10]))\n",
    "  print(label3[:10])\n",
    "  label0 = (np.arange(num_digits) == label0[:,None]).astype(np.float32)\n",
    "  label1 = (np.arange(num_labels) == label1[:,None]).astype(np.float32)\n",
    "  label2 = (np.arange(num_labels) == label2[:,None]).astype(np.float32)\n",
    "  label3 = (np.arange(num_labels) == label3[:,None]).astype(np.float32)\n",
    "  label4 = (np.arange(num_labels) == label4[:,None]).astype(np.float32)\n",
    "  label5 = (np.arange(num_labels) == label5[:,None]).astype(np.float32)\n",
    "\n",
    "  print(label3[:10])\n",
    "  return dataset, label0, label1, label2, label3, label4, label5\n",
    "train_dataset, train_label0, train_label1, train_label2, train_label3, train_label4, train_label5 = reformat(np.asarray(X_train), L_train, y1_train, y2_train, y3_train, y4_train, y5_train)\n",
    "valid_dataset, valid_label0, valid_label1, valid_label2, valid_label3, valid_label4, valid_label5 = reformat(np.asarray(X_valid), L_valid, y1_valid, y2_valid, y3_valid, y4_valid, y5_valid)\n",
    "#test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "#normalize to 0 mean and unit variance\n",
    "train_dataset_norm = (train_dataset - np.mean( train_dataset))/np.std( train_dataset)\n",
    "\n",
    "# Check values\n",
    "print('Training set', train_dataset.shape, train_label0.shape, train_label1.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_label0.shape, valid_label1.shape)\n",
    "#print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot house number image\n",
    "\n",
    "#### Reference: https://github.com/llSourcell/How_to_make_a_tensorflow_image_classifier_LIVE/blob/master/demonotes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19', '23', '25', '93', '31', '33', '28', '744', '128']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAD5CAYAAAC9FVegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133a4bc49b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    #assert len(images) == len(cls_true) \n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(image_size, image_size), cmap='binary')\n",
    "        \n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "    \n",
    "# Get the first images from the test-set.\n",
    "images = train_dataset_norm[0:9]\n",
    "    \n",
    "# Get the true classes for those images.\n",
    "cls_true = [\"{}{}{}{}{}\".format(str(a_).replace('10',''), str(b_).replace('10',''), str(c_).replace('10',''), str(d_).replace('10',''), str(e_).replace('10','')) \n",
    "       for a_, b_, c_, d_, e_ in zip(y1_train[0:9], y2_train[0:9], y3_train[0:9], y4_train[0:9], y5_train[0:9])]\n",
    "\n",
    "print([\"{}{}{}{}{}\".format(str(a_).replace('10',''), str(b_).replace('10',''), str(c_).replace('10',''), str(d_).replace('10',''), str(e_).replace('10','')) \n",
    "       for a_, b_, c_, d_, e_ in zip(y1_train[0:9], y2_train[0:9], y3_train[0:9], y4_train[0:9], y5_train[0:9])])\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "valid_dataset_batch = valid_dataset[:batch_size]\n",
    "\n",
    "num_conv_layers = 3\n",
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "#Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36        # There are 36 of these filters.\n",
    "\n",
    "#Convolutional Layer 2.\n",
    "filter_size3 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters3 = 64        # There are 64 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 1024         # Number of neurons in fully-connected hidden layer\n",
    "num_fc_layers = 3\n",
    "drop_rate1 = 0.4 \n",
    "drop_rate2 = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare variables to initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    #with tf.device('/gpu:0'):\n",
    "    \n",
    "  def weight_variable(w_id, shape, stdev_num):\n",
    "    #return (tf.Variable(tf.truncated_normal(shape,stddev=np.sqrt(2.0/(stdev_num)))))\n",
    "    return (tf.get_variable(w_id,  initializer = tf.truncated_normal(shape,stddev=np.sqrt(2.0/(stdev_num)))))\n",
    "\n",
    "  def bias_variable(b_id, shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    #return tf.Variable(initial)\n",
    "    return tf.get_variable(b_id, initializer = initial)\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels0 = tf.placeholder(tf.float32, shape=(batch_size, num_digits))\n",
    "  tf_train_labels1 = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_train_labels2 = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_train_labels3 = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_train_labels4 = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_train_labels5 = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset =  tf.constant(valid_dataset_batch) \n",
    " # tf_test_dataset = tf.constant(test_dataset) \n",
    "\n",
    "  def conv_layers(layer_num, data, weights, biases, input_size,filter_size, num_filters, valid=False, use_pooling=True): \n",
    "   # ########\n",
    "   # # Conv with leaky relu and 2x2 max pooling\n",
    "   # ########\n",
    "\n",
    "    if not valid:\n",
    "    # create filter-weights and bias during training\n",
    "        shape = [filter_size, filter_size, input_size, num_filters]\n",
    "        stdev_n = (filter_size*filter_size*input_size)\n",
    "        weights.append(weight_variable(\"cvw0_\"+layer_num, shape, stdev_n))\n",
    "        biases.append(bias_variable(\"cvb0_\"+layer_num, [num_filters]))\n",
    "\n",
    "    hidden=[]\n",
    "    y0 =  tf.nn.conv2d(input=data,filter=weights[0],strides=[1, 1, 1, 1],padding='SAME')\n",
    "    hidden.append(tf.maximum((y0 + biases[0]), 0.01*(y0+biases[0])))\n",
    "    \n",
    "    for L in range(1,num_conv_layers+1):   \n",
    "        if not valid:\n",
    "            shape = [filter_size, filter_size, num_filters, num_filters]\n",
    "            stdev_n = filter_size*filter_size*num_filters\n",
    "                \n",
    "            weights.append(weight_variable(\"cvw\"+str(L)+\"_\"+layer_num, shape, stdev_n))\n",
    "            biases.append(bias_variable(\"cvb\"+str(L)+\"_\"+layer_num, [num_filters]))   \n",
    "\n",
    "        y = tf.nn.conv2d(input=hidden[L-1],filter=weights[L],strides=[1, 1, 1, 1],padding='SAME')\n",
    "        hidden.append(tf.maximum((y + biases[L]), 0.01*(y+biases[L])))\n",
    "\n",
    "    layer = tf.nn.max_pool(value=hidden[num_conv_layers], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    return layer, weights, biases\n",
    "    \n",
    "  def fc_layers(layer_num, weights, biases, out_weights, out_biases, data, label_count, valid = False):\n",
    "   #  #####\n",
    "   #  # FC Layers - with leaky relu and dropout\n",
    "   #  #####\n",
    "\n",
    "    out_size = data.get_shape().as_list()[1]\n",
    "    if not valid:\n",
    "        #create weights and biases during training\n",
    "        weights.append(weight_variable(\"fcw0_\"+layer_num, [out_size, fc_size], fc_size*fc_size)) # w = 1024 depth * 1024 FC hidden node layer\n",
    "        biases.append(bias_variable(\"fcb0_\"+layer_num, [fc_size])) #  # b = 1024 FC hidden node layers1024\n",
    "\n",
    "    hidden=[]\n",
    "    y0 = (tf.matmul(data, weights[0]) + biases[0])\n",
    "    hidden.append(tf.nn.dropout(tf.maximum(y0, 0.01*y0), drop_rate1))\n",
    "    \n",
    "    for L in range(1,num_fc_layers+1):   \n",
    "        if not valid:\n",
    "            weights.append(weight_variable(\"fcw\"+str(L)+\"_\"+layer_num,[fc_size, fc_size], fc_size*fc_size))\n",
    "            biases.append(bias_variable(\"fcb\"+str(L)+\"_\"+layer_num,[fc_size]))   \n",
    "        y = ((tf.matmul(hidden[L-1], weights[L]) + biases[L])) \n",
    "        hidden.append(tf.nn.dropout(tf.maximum(y, 0.01*y), drop_rate2))\n",
    "        \n",
    "      #Definitive output  after relu layer's:  \n",
    "    if not valid:\n",
    "        out_weights = weight_variable(\"rlw_\"+layer_num,[fc_size, label_count], fc_size*label_count)\n",
    "        out_biases = bias_variable(\"rlb_\"+layer_num,[label_count])      \n",
    "    logits = tf.matmul(hidden[num_fc_layers], out_weights) + out_biases\n",
    "        \n",
    "    return logits, weights, biases, out_weights, out_biases\n",
    "    \n",
    "  wc1, wc2, wc3 = [], [], []\n",
    "  bc1, bc2, bc3 = [], [], []\n",
    "  convpool1, wc1, bc1 = conv_layers(\"1\", tf_train_dataset, wc1, bc1, num_channels, filter_size1, num_filters1)\n",
    "  convpool2, wc2, bc2 = conv_layers(\"2\", convpool1, wc2, bc2, num_filters1, filter_size2, num_filters2)\n",
    "  convpool3, wc3, bc3 = conv_layers(\"3\", convpool2, wc3, bc3, num_filters2, filter_size3, num_filters3)\n",
    "\n",
    "  shape = convpool3.get_shape().as_list()\n",
    "  conv_layers_reshape = tf.reshape(convpool3, [shape[0], shape[1] * shape[2] * shape[3]]) #[256, 4, 4, 64]\n",
    "\n",
    "  w0, w1, w2, w3, w4, w5 = [], [], [], [], [], []\n",
    "  b0, b1, b2, b3, b4, b5 = [], [], [], [], [], []\n",
    "  \n",
    " # Training computation.\n",
    "  logits0, w0, b0, out_w0, out_b0 = fc_layers(\"1\", w0, b0, None, None, conv_layers_reshape, num_digits)  \n",
    "  logits1, w1, b1, out_w1, out_b1 = fc_layers(\"2\", w1, b1, None, None, conv_layers_reshape, num_labels) \n",
    "  logits2, w2, b2, out_w2, out_b2 = fc_layers(\"3\", w2, b2, None, None, conv_layers_reshape, num_labels) \n",
    "  logits3, w3, b3, out_w3, out_b3 = fc_layers(\"4\", w3, b3, None, None, conv_layers_reshape, num_labels) \n",
    "  logits4, w4, b4, out_w4, out_b4 = fc_layers(\"5\", w4, b4, None, None, conv_layers_reshape, num_labels) \n",
    "  logits5, w5, b5, out_w5, out_b5 = fc_layers(\"6\", w5, b5, None, None, conv_layers_reshape, num_labels)   \n",
    "\n",
    "  loss0 = tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels0, logits=logits0) \n",
    "  loss1 = tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels1, logits=logits1)\n",
    "  loss2 = tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels2, logits=logits2)\n",
    "  loss3 = tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels3, logits=logits3)\n",
    "  loss4 = tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels4, logits=logits4)\n",
    "  loss5 = tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels5, logits=logits5)\n",
    "  loss= tf.reduce_mean(loss0) + tf.reduce_mean(loss1) + tf.reduce_mean(loss2) + tf.reduce_mean(loss3) + tf.reduce_mean(loss4) + tf.reduce_mean(loss5)\n",
    "    \n",
    " # Optimizer.\n",
    " # optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  global_step = tf.Variable(0) #, trainable=False)  # count the number of steps taken.\n",
    " # learning_rate = tf.train.exponential_decay(0.1, global_step, 100000, 0.96, staircase=True)\n",
    "#  learning_rate = tf.train.exponential_decay(0.05, global_step, 100, 0.95) #, staircase=True)\n",
    " # optimizer = tf.train.GradientDescentOptimizer(1e-6).minimize(loss) #, global_step=global_step)\n",
    "  optimizer =tf.train.AdamOptimizer(0.001).minimize(loss, global_step=global_step) \n",
    "\n",
    "  # Validation computation.\n",
    "  convpool1v, _, _ = conv_layers(\"1\", tf_valid_dataset, wc1, bc1, num_channels, filter_size1, num_filters1, True)\n",
    "  convpool2v, _, _ = conv_layers(\"2\", convpool1v, wc2, bc2, num_filters1, filter_size2, num_filters2, True)\n",
    "  convpool3v, _, _ = conv_layers(\"3\", convpool2v, wc3, bc3, num_filters2, filter_size3, num_filters3, True)\n",
    "  shapev = convpool3v.get_shape().as_list()\n",
    "  conv_layers_reshape_valid = tf.reshape(convpool3v, [shapev[0], shapev[1] * shapev[2] * shapev[3]]) #[256, 4, 4, 64]\n",
    "\n",
    "  logits0_valid, _, _, _, _ = fc_layers(\"1\", w0, b0, out_w0, out_b0, conv_layers_reshape_valid, num_digits, True) \n",
    "  logits1_valid, _, _, _, _ = fc_layers(\"2\", w1, b1, out_w1, out_b1, conv_layers_reshape_valid, num_labels, True) \n",
    "  logits2_valid, _, _, _, _ = fc_layers(\"3\", w2, b2, out_w2, out_b2, conv_layers_reshape_valid, num_labels, True) \n",
    "  logits3_valid, _, _, _, _ = fc_layers(\"4\", w3, b3, out_w3, out_b3, conv_layers_reshape_valid, num_labels, True) \n",
    "  logits4_valid, _, _, _, _ = fc_layers(\"5\", w4, b4, out_w4, out_b4, conv_layers_reshape_valid, num_labels, True) \n",
    "  logits5_valid, _, _, _, _ = fc_layers(\"6\", w5, b5, out_w5, out_b5, conv_layers_reshape_valid, num_labels, True) \n",
    "\n",
    " # Predictions for the training, validation, and test data.\n",
    "  train_prediction0 = tf.nn.softmax(logits0)\n",
    "  valid_prediction0 = tf.nn.softmax(logits0_valid)\n",
    "  train_prediction1 = tf.nn.softmax(logits1)\n",
    "  valid_prediction1 = tf.nn.softmax(logits1_valid)\n",
    "  train_prediction2 = tf.nn.softmax(logits2)\n",
    "  valid_prediction2 = tf.nn.softmax(logits2_valid)\n",
    "  train_prediction3 = tf.nn.softmax(logits3)\n",
    "  valid_prediction3 = tf.nn.softmax(logits3_valid)\n",
    "  train_prediction4 = tf.nn.softmax(logits4)\n",
    "  valid_prediction4 = tf.nn.softmax(logits4_valid)\n",
    "  train_prediction5 = tf.nn.softmax(logits5)\n",
    "  valid_prediction5 = tf.nn.softmax(logits5_valid)\n",
    "\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset)) \n",
    "\n",
    "  # Add ops to save and restore all the variables.\n",
    "  saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Run graph session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch training loss at step 0: 13.777848\n",
      "Training accuracy - digit count: 3.1%; digits 1-5: 6.6%, 6.2%, 4.3%, 2.0%, 2.0%\n",
      "Training avg accuracy: 4.0%\n",
      "Validation accuracy - digit count: 3.1%; digits 1-5: 11.3%, 8.6%, 4.7%, 2.0%, 1.6%\n",
      "Validation avg accuracy: 5.2%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 50: 7.124750\n",
      "Training accuracy - digit count: 49.6%; digits 1-5: 25.0%, 19.1%, 68.8%, 96.9%, 100.0%\n",
      "Training avg accuracy: 59.9%\n",
      "Validation accuracy - digit count: 48.8%; digits 1-5: 27.3%, 18.4%, 67.2%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 59.6%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 100: 6.987835\n",
      "Training accuracy - digit count: 53.9%; digits 1-5: 27.7%, 19.1%, 73.0%, 96.1%, 100.0%\n",
      "Training avg accuracy: 61.7%\n",
      "Validation accuracy - digit count: 48.8%; digits 1-5: 27.3%, 18.4%, 67.2%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 59.6%\n",
      "Minibatch training loss at step 150: 6.644724\n",
      "Training accuracy - digit count: 59.8%; digits 1-5: 30.9%, 18.8%, 72.7%, 94.9%, 100.0%\n",
      "Training avg accuracy: 62.8%\n",
      "Validation accuracy - digit count: 61.3%; digits 1-5: 32.0%, 25.0%, 67.2%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 63.6%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 200: 5.910306\n",
      "Training accuracy - digit count: 71.9%; digits 1-5: 39.8%, 23.0%, 73.0%, 96.9%, 100.0%\n",
      "Training avg accuracy: 67.4%\n",
      "Validation accuracy - digit count: 75.4%; digits 1-5: 37.1%, 27.3%, 67.6%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 67.3%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 250: 5.790462\n",
      "Training accuracy - digit count: 71.1%; digits 1-5: 42.2%, 30.5%, 70.3%, 95.3%, 100.0%\n",
      "Training avg accuracy: 68.2%\n",
      "Validation accuracy - digit count: 76.2%; digits 1-5: 40.2%, 26.2%, 68.0%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 67.8%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 300: 4.849355\n",
      "Training accuracy - digit count: 84.0%; digits 1-5: 51.2%, 32.8%, 68.4%, 96.5%, 100.0%\n",
      "Training avg accuracy: 72.1%\n",
      "Validation accuracy - digit count: 85.9%; digits 1-5: 50.0%, 37.1%, 69.1%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 73.0%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 350: 4.274024\n",
      "Training accuracy - digit count: 89.5%; digits 1-5: 59.8%, 33.6%, 76.6%, 96.1%, 100.0%\n",
      "Training avg accuracy: 75.9%\n",
      "Validation accuracy - digit count: 88.7%; digits 1-5: 56.2%, 41.4%, 70.3%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 75.5%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 400: 4.035082\n",
      "Training accuracy - digit count: 87.9%; digits 1-5: 62.1%, 44.1%, 70.7%, 94.1%, 100.0%\n",
      "Training avg accuracy: 76.5%\n",
      "Validation accuracy - digit count: 89.1%; digits 1-5: 60.2%, 45.3%, 68.8%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 76.6%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 450: 3.577110\n",
      "Training accuracy - digit count: 87.9%; digits 1-5: 69.9%, 48.4%, 72.3%, 96.9%, 100.0%\n",
      "Training avg accuracy: 79.2%\n",
      "Validation accuracy - digit count: 88.3%; digits 1-5: 66.0%, 54.7%, 70.7%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 79.3%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 500: 3.571562\n",
      "Training accuracy - digit count: 82.0%; digits 1-5: 73.4%, 52.0%, 76.2%, 94.1%, 100.0%\n",
      "Training avg accuracy: 79.6%\n",
      "Validation accuracy - digit count: 87.5%; digits 1-5: 72.3%, 56.2%, 69.9%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 80.3%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 550: 3.386100\n",
      "Training accuracy - digit count: 92.2%; digits 1-5: 76.6%, 60.5%, 69.1%, 95.7%, 100.0%\n",
      "Training avg accuracy: 82.4%\n",
      "Validation accuracy - digit count: 89.8%; digits 1-5: 72.3%, 60.5%, 70.3%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 81.5%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 600: 2.600194\n",
      "Training accuracy - digit count: 94.1%; digits 1-5: 79.3%, 65.2%, 77.0%, 96.1%, 100.0%\n",
      "Training avg accuracy: 85.3%\n",
      "Validation accuracy - digit count: 89.8%; digits 1-5: 77.0%, 64.8%, 71.1%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 83.1%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 650: 2.612749\n",
      "Training accuracy - digit count: 90.6%; digits 1-5: 81.2%, 71.9%, 72.3%, 96.9%, 100.0%\n",
      "Training avg accuracy: 85.5%\n",
      "Validation accuracy - digit count: 90.2%; digits 1-5: 77.7%, 71.1%, 71.1%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 84.4%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 700: 2.477888\n",
      "Training accuracy - digit count: 93.8%; digits 1-5: 84.0%, 74.2%, 70.7%, 94.9%, 100.0%\n",
      "Training avg accuracy: 86.3%\n",
      "Validation accuracy - digit count: 93.0%; digits 1-5: 81.6%, 69.5%, 69.1%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 84.9%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 750: 2.386771\n",
      "Training accuracy - digit count: 92.6%; digits 1-5: 87.5%, 78.9%, 76.6%, 94.9%, 100.0%\n",
      "Training avg accuracy: 88.4%\n",
      "Validation accuracy - digit count: 87.5%; digits 1-5: 81.2%, 75.4%, 71.1%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 85.2%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 800: 2.503408\n",
      "Training accuracy - digit count: 91.8%; digits 1-5: 85.2%, 73.4%, 75.4%, 94.5%, 100.0%\n",
      "Training avg accuracy: 86.7%\n",
      "Validation accuracy - digit count: 90.6%; digits 1-5: 82.0%, 78.1%, 70.3%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 86.3%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 850: 2.117370\n",
      "Training accuracy - digit count: 93.4%; digits 1-5: 91.0%, 80.5%, 73.8%, 95.3%, 100.0%\n",
      "Training avg accuracy: 89.0%\n",
      "Validation accuracy - digit count: 91.4%; digits 1-5: 83.6%, 77.7%, 72.7%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 87.0%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 900: 1.835537\n",
      "Training accuracy - digit count: 97.3%; digits 1-5: 90.2%, 78.9%, 77.3%, 96.1%, 100.0%\n",
      "Training avg accuracy: 90.0%\n",
      "Validation accuracy - digit count: 90.6%; digits 1-5: 84.4%, 73.8%, 73.0%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 86.3%\n",
      "Minibatch training loss at step 950: 2.098379\n",
      "Training accuracy - digit count: 94.1%; digits 1-5: 89.1%, 80.9%, 71.9%, 94.9%, 100.0%\n",
      "Training avg accuracy: 88.5%\n",
      "Validation accuracy - digit count: 92.2%; digits 1-5: 82.8%, 76.2%, 73.0%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 86.8%\n",
      "Minibatch training loss at step 1000: 1.998744\n",
      "Training accuracy - digit count: 93.4%; digits 1-5: 88.3%, 82.0%, 76.6%, 96.5%, 100.0%\n",
      "Training avg accuracy: 89.5%\n",
      "Validation accuracy - digit count: 91.8%; digits 1-5: 86.3%, 76.6%, 75.0%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 87.7%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 1050: 1.711851\n",
      "Training accuracy - digit count: 96.1%; digits 1-5: 93.4%, 83.2%, 76.2%, 94.1%, 100.0%\n",
      "Training avg accuracy: 90.5%\n",
      "Validation accuracy - digit count: 92.2%; digits 1-5: 85.9%, 78.9%, 71.5%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 87.5%\n",
      "Minibatch training loss at step 1100: 1.643280\n",
      "Training accuracy - digit count: 95.3%; digits 1-5: 93.0%, 81.6%, 83.2%, 97.3%, 100.0%\n",
      "Training avg accuracy: 91.7%\n",
      "Validation accuracy - digit count: 91.0%; digits 1-5: 86.7%, 74.6%, 71.5%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 86.7%\n",
      "Minibatch training loss at step 1150: 1.672349\n",
      "Training accuracy - digit count: 96.9%; digits 1-5: 88.7%, 82.4%, 78.9%, 97.3%, 100.0%\n",
      "Training avg accuracy: 90.7%\n",
      "Validation accuracy - digit count: 93.4%; digits 1-5: 84.8%, 81.2%, 75.8%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 88.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 1200: 1.618898\n",
      "Training accuracy - digit count: 97.3%; digits 1-5: 93.0%, 81.6%, 80.1%, 94.5%, 100.0%\n",
      "Training avg accuracy: 91.1%\n",
      "Validation accuracy - digit count: 91.8%; digits 1-5: 85.2%, 79.3%, 74.2%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 87.8%\n",
      "Minibatch training loss at step 1250: 1.384715\n",
      "Training accuracy - digit count: 98.0%; digits 1-5: 92.2%, 87.9%, 82.8%, 96.5%, 99.6%\n",
      "Training avg accuracy: 92.8%\n",
      "Validation accuracy - digit count: 93.4%; digits 1-5: 87.9%, 78.5%, 73.4%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 88.3%\n",
      "Minibatch training loss at step 1300: 1.507797\n",
      "Training accuracy - digit count: 96.1%; digits 1-5: 94.5%, 87.9%, 78.9%, 96.1%, 100.0%\n",
      "Training avg accuracy: 92.3%\n",
      "Validation accuracy - digit count: 91.8%; digits 1-5: 87.9%, 78.5%, 72.3%, 94.5%, 100.0%\n",
      "Validation avg accuracy: 87.5%\n",
      "Minibatch training loss at step 1350: 1.628713\n",
      "Training accuracy - digit count: 96.1%; digits 1-5: 94.1%, 82.4%, 82.0%, 93.8%, 100.0%\n",
      "Training avg accuracy: 91.4%\n",
      "Validation accuracy - digit count: 89.5%; digits 1-5: 82.4%, 78.5%, 74.2%, 95.3%, 100.0%\n",
      "Validation avg accuracy: 86.7%\n",
      "Minibatch training loss at step 1400: 1.658967\n",
      "Training accuracy - digit count: 96.5%; digits 1-5: 87.1%, 85.5%, 82.4%, 97.7%, 100.0%\n",
      "Training avg accuracy: 91.5%\n",
      "Validation accuracy - digit count: 89.5%; digits 1-5: 81.2%, 77.7%, 73.8%, 94.9%, 100.0%\n",
      "Validation avg accuracy: 86.2%\n",
      "Minibatch training loss at step 1450: 1.603331\n",
      "Training accuracy - digit count: 97.3%; digits 1-5: 94.5%, 86.3%, 81.2%, 94.1%, 99.6%\n",
      "Training avg accuracy: 92.2%\n",
      "Validation accuracy - digit count: 92.2%; digits 1-5: 86.7%, 81.2%, 77.0%, 95.7%, 100.0%\n",
      "Validation avg accuracy: 88.8%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 1500: 1.642603\n",
      "Training accuracy - digit count: 96.9%; digits 1-5: 90.6%, 85.9%, 79.7%, 93.4%, 100.0%\n",
      "Training avg accuracy: 91.1%\n",
      "Validation accuracy - digit count: 89.8%; digits 1-5: 84.4%, 79.7%, 77.0%, 95.3%, 100.0%\n",
      "Validation avg accuracy: 87.7%\n",
      "Minibatch training loss at step 1550: 1.463758\n",
      "Training accuracy - digit count: 94.5%; digits 1-5: 92.2%, 85.5%, 83.6%, 95.3%, 100.0%\n",
      "Training avg accuracy: 91.9%\n",
      "Validation accuracy - digit count: 89.8%; digits 1-5: 83.6%, 76.2%, 74.6%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 86.7%\n",
      "Minibatch training loss at step 1600: 1.562174\n",
      "Training accuracy - digit count: 96.9%; digits 1-5: 92.6%, 87.1%, 79.3%, 94.5%, 100.0%\n",
      "Training avg accuracy: 91.7%\n",
      "Validation accuracy - digit count: 91.4%; digits 1-5: 85.5%, 78.9%, 76.2%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 88.0%\n",
      "Minibatch training loss at step 1650: 1.268782\n",
      "Training accuracy - digit count: 98.8%; digits 1-5: 94.1%, 88.7%, 83.6%, 98.4%, 100.0%\n",
      "Training avg accuracy: 93.9%\n",
      "Validation accuracy - digit count: 89.5%; digits 1-5: 85.9%, 80.5%, 77.7%, 95.7%, 100.0%\n",
      "Validation avg accuracy: 88.2%\n",
      "Minibatch training loss at step 1700: 1.144115\n",
      "Training accuracy - digit count: 97.7%; digits 1-5: 94.9%, 88.7%, 84.8%, 96.1%, 100.0%\n",
      "Training avg accuracy: 93.7%\n",
      "Validation accuracy - digit count: 91.0%; digits 1-5: 85.2%, 79.7%, 77.3%, 94.9%, 100.0%\n",
      "Validation avg accuracy: 88.0%\n",
      "Minibatch training loss at step 1750: 1.077328\n",
      "Training accuracy - digit count: 99.2%; digits 1-5: 94.1%, 85.2%, 89.5%, 96.9%, 100.0%\n",
      "Training avg accuracy: 94.1%\n",
      "Validation accuracy - digit count: 91.0%; digits 1-5: 85.2%, 79.7%, 75.8%, 95.7%, 100.0%\n",
      "Validation avg accuracy: 87.9%\n",
      "Minibatch training loss at step 1800: 1.094386\n",
      "Training accuracy - digit count: 98.4%; digits 1-5: 94.1%, 91.0%, 89.5%, 94.5%, 99.6%\n",
      "Training avg accuracy: 94.5%\n",
      "Validation accuracy - digit count: 90.2%; digits 1-5: 83.6%, 80.1%, 77.3%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 87.9%\n",
      "Minibatch training loss at step 1850: 1.111690\n",
      "Training accuracy - digit count: 98.0%; digits 1-5: 92.6%, 87.5%, 84.8%, 98.0%, 100.0%\n",
      "Training avg accuracy: 93.5%\n",
      "Validation accuracy - digit count: 91.0%; digits 1-5: 84.4%, 80.1%, 77.0%, 95.7%, 100.0%\n",
      "Validation avg accuracy: 88.0%\n",
      "Minibatch training loss at step 1900: 0.975410\n",
      "Training accuracy - digit count: 97.7%; digits 1-5: 93.4%, 90.2%, 89.8%, 96.9%, 100.0%\n",
      "Training avg accuracy: 94.7%\n",
      "Validation accuracy - digit count: 92.2%; digits 1-5: 83.6%, 82.0%, 77.0%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 88.5%\n",
      "Minibatch training loss at step 1950: 1.174774\n",
      "Training accuracy - digit count: 97.3%; digits 1-5: 94.1%, 88.7%, 83.2%, 97.3%, 100.0%\n",
      "Training avg accuracy: 93.4%\n",
      "Validation accuracy - digit count: 90.6%; digits 1-5: 84.4%, 78.5%, 78.1%, 93.8%, 100.0%\n",
      "Validation avg accuracy: 87.6%\n",
      "Minibatch training loss at step 2000: 1.193291\n",
      "Training accuracy - digit count: 96.1%; digits 1-5: 95.3%, 87.1%, 87.9%, 94.5%, 100.0%\n",
      "Training avg accuracy: 93.5%\n",
      "Validation accuracy - digit count: 91.4%; digits 1-5: 86.3%, 76.2%, 76.6%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 87.8%\n",
      "Minibatch training loss at step 2050: 0.980094\n",
      "Training accuracy - digit count: 98.4%; digits 1-5: 94.1%, 91.8%, 90.6%, 97.7%, 100.0%\n",
      "Training avg accuracy: 95.4%\n",
      "Validation accuracy - digit count: 90.2%; digits 1-5: 85.5%, 76.6%, 78.1%, 94.1%, 100.0%\n",
      "Validation avg accuracy: 87.4%\n",
      "Minibatch training loss at step 2100: 1.103884\n",
      "Training accuracy - digit count: 97.3%; digits 1-5: 94.9%, 90.2%, 87.9%, 93.8%, 100.0%\n",
      "Training avg accuracy: 94.0%\n",
      "Validation accuracy - digit count: 91.0%; digits 1-5: 86.7%, 79.7%, 81.2%, 95.3%, 100.0%\n",
      "Validation avg accuracy: 89.0%\n",
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 2150: 1.022787\n",
      "Training accuracy - digit count: 97.7%; digits 1-5: 94.1%, 92.6%, 89.1%, 98.0%, 100.0%\n",
      "Training avg accuracy: 95.2%\n",
      "Validation accuracy - digit count: 89.5%; digits 1-5: 86.3%, 80.9%, 76.6%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 88.2%\n",
      "Minibatch training loss at step 2200: 1.220722\n",
      "Training accuracy - digit count: 96.5%; digits 1-5: 93.4%, 87.9%, 88.3%, 94.9%, 100.0%\n",
      "Training avg accuracy: 93.5%\n",
      "Validation accuracy - digit count: 88.3%; digits 1-5: 85.2%, 78.9%, 79.7%, 94.5%, 100.0%\n",
      "Validation avg accuracy: 87.8%\n",
      "Minibatch training loss at step 2250: 0.862025\n",
      "Training accuracy - digit count: 99.2%; digits 1-5: 93.0%, 90.2%, 93.0%, 95.7%, 100.0%\n",
      "Training avg accuracy: 95.2%\n",
      "Validation accuracy - digit count: 91.4%; digits 1-5: 83.6%, 79.7%, 79.7%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 88.5%\n",
      "Minibatch training loss at step 2300: 0.980636\n",
      "Training accuracy - digit count: 98.0%; digits 1-5: 94.1%, 92.2%, 85.9%, 97.3%, 100.0%\n",
      "Training avg accuracy: 94.6%\n",
      "Validation accuracy - digit count: 89.8%; digits 1-5: 84.0%, 79.3%, 78.5%, 94.9%, 100.0%\n",
      "Validation avg accuracy: 87.8%\n",
      "Minibatch training loss at step 2350: 1.127935\n",
      "Training accuracy - digit count: 97.7%; digits 1-5: 93.8%, 91.0%, 89.1%, 95.3%, 100.0%\n",
      "Training avg accuracy: 94.5%\n",
      "Validation accuracy - digit count: 92.6%; digits 1-5: 85.5%, 78.9%, 78.5%, 95.7%, 100.0%\n",
      "Validation avg accuracy: 88.5%\n",
      "Minibatch training loss at step 2400: 0.868729\n",
      "Training accuracy - digit count: 98.8%; digits 1-5: 94.9%, 91.0%, 92.2%, 95.7%, 100.0%\n",
      "Training avg accuracy: 95.4%\n",
      "Validation accuracy - digit count: 90.6%; digits 1-5: 83.6%, 80.5%, 82.0%, 95.3%, 100.0%\n",
      "Validation avg accuracy: 88.7%\n",
      "Minibatch training loss at step 2450: 1.048962\n",
      "Training accuracy - digit count: 98.4%; digits 1-5: 95.7%, 91.4%, 88.3%, 96.5%, 100.0%\n",
      "Training avg accuracy: 95.1%\n",
      "Validation accuracy - digit count: 90.6%; digits 1-5: 83.6%, 81.6%, 78.5%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 88.4%\n",
      "Minibatch training loss at step 2500: 0.890118\n",
      "Training accuracy - digit count: 96.1%; digits 1-5: 94.5%, 91.8%, 92.2%, 97.7%, 100.0%\n",
      "Training avg accuracy: 95.4%\n",
      "Validation accuracy - digit count: 89.5%; digits 1-5: 84.4%, 81.2%, 80.9%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 88.7%\n",
      "Minibatch training loss at step 2550: 1.007670\n",
      "Training accuracy - digit count: 98.8%; digits 1-5: 94.5%, 92.6%, 90.2%, 93.8%, 100.0%\n",
      "Training avg accuracy: 95.0%\n",
      "Validation accuracy - digit count: 91.4%; digits 1-5: 86.7%, 81.2%, 81.6%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 89.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Minibatch training loss at step 2600: 0.752590\n",
      "Training accuracy - digit count: 99.2%; digits 1-5: 96.5%, 89.5%, 93.0%, 97.7%, 100.0%\n",
      "Training avg accuracy: 96.0%\n",
      "Validation accuracy - digit count: 89.5%; digits 1-5: 83.6%, 77.7%, 81.2%, 95.7%, 100.0%\n",
      "Validation avg accuracy: 88.0%\n",
      "Minibatch training loss at step 2650: 0.811610\n",
      "Training accuracy - digit count: 98.8%; digits 1-5: 94.9%, 92.6%, 93.4%, 96.1%, 100.0%\n",
      "Training avg accuracy: 96.0%\n",
      "Validation accuracy - digit count: 90.6%; digits 1-5: 85.2%, 81.2%, 80.5%, 95.3%, 100.0%\n",
      "Validation avg accuracy: 88.8%\n",
      "Minibatch training loss at step 2700: 0.842172\n",
      "Training accuracy - digit count: 98.8%; digits 1-5: 96.1%, 89.5%, 90.2%, 97.3%, 100.0%\n",
      "Training avg accuracy: 95.3%\n",
      "Validation accuracy - digit count: 91.4%; digits 1-5: 84.8%, 80.1%, 80.9%, 96.1%, 100.0%\n",
      "Validation avg accuracy: 88.9%\n",
      "Minibatch training loss at step 2750: 0.670006\n",
      "Training accuracy - digit count: 99.6%; digits 1-5: 94.9%, 93.4%, 92.6%, 97.7%, 100.0%\n",
      "Training avg accuracy: 96.4%\n",
      "Validation accuracy - digit count: 89.1%; digits 1-5: 85.5%, 82.4%, 81.2%, 95.3%, 100.0%\n",
      "Validation avg accuracy: 88.9%\n",
      "Minibatch training loss at step 2800: 0.644953\n",
      "Training accuracy - digit count: 98.8%; digits 1-5: 98.0%, 94.9%, 93.0%, 96.1%, 100.0%\n",
      "Training avg accuracy: 96.8%\n",
      "Validation accuracy - digit count: 90.6%; digits 1-5: 84.4%, 79.7%, 81.6%, 96.5%, 100.0%\n",
      "Validation avg accuracy: 88.8%\n",
      "Minibatch training loss at step 2850: 0.820977\n",
      "Training accuracy - digit count: 98.4%; digits 1-5: 95.3%, 92.2%, 90.6%, 96.5%, 99.6%\n",
      "Training avg accuracy: 95.4%\n",
      "Validation accuracy - digit count: 89.5%; digits 1-5: 84.4%, 80.1%, 81.6%, 94.1%, 100.0%\n",
      "Validation avg accuracy: 88.3%\n",
      "Minibatch training loss at step 2900: 0.681599\n",
      "Training accuracy - digit count: 99.6%; digits 1-5: 96.1%, 91.8%, 92.2%, 99.2%, 100.0%\n",
      "Training avg accuracy: 96.5%\n",
      "Validation accuracy - digit count: 91.8%; digits 1-5: 83.2%, 77.7%, 81.6%, 95.3%, 100.0%\n",
      "Validation avg accuracy: 88.3%\n",
      "Minibatch training loss at step 2950: 0.789870\n",
      "Training accuracy - digit count: 98.8%; digits 1-5: 95.3%, 91.4%, 91.4%, 95.3%, 100.0%\n",
      "Training avg accuracy: 95.4%\n",
      "Validation accuracy - digit count: 90.2%; digits 1-5: 84.8%, 81.6%, 80.5%, 94.9%, 100.0%\n",
      "Validation avg accuracy: 88.7%\n",
      "Minibatch training loss at step 3000: 0.947978\n",
      "Training accuracy - digit count: 96.9%; digits 1-5: 94.9%, 93.4%, 93.0%, 96.1%, 100.0%\n",
      "Training avg accuracy: 95.7%\n",
      "Validation accuracy - digit count: 91.4%; digits 1-5: 84.0%, 80.1%, 79.7%, 95.7%, 100.0%\n",
      "Validation avg accuracy: 88.5%\n",
      "Time: 30936.89s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "#def eval_accuracy(eval_l_preds, eval_preds, l_labels, labels, masks):\n",
    "#    concatted = np.concatenate((np.reshape((eval_l_preds == l_labels), [-1, 1]), \n",
    "#                                (eval_preds * masks) == labels), axis=1)\n",
    "#    return 100.0 * (np.sum([np.all(row) for row in concatted])) / len(labels)\n",
    "#\n",
    "\n",
    "num_steps = 3001\n",
    "max_acc = 0\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run() # sess.run(tf.global_variables_initializer())\n",
    "  #session.run(init_op)\n",
    "  print('Initialized')\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (np.asarray(y1_train).shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :] #, :, :]\n",
    "    \n",
    "    #normalize data to mean 0 and unit variance\n",
    "   # batch_data = (batch_data - np.mean(batch_data))/np.std(batch_data)\n",
    "    #batch_data = np.reshape(batch_data,(batch_size,image_size*image_size))\n",
    "   # print(batch_data.shape)\n",
    "\n",
    "  #  batch_data_valid = valid_dataset[offset:(offset + batch_size), :] #, :, :]\n",
    "\n",
    "    batch_labels0 =  np.asarray(train_label0)[offset:(offset + batch_size), :]\n",
    "    batch_labels1 =  np.asarray(train_label1)[offset:(offset + batch_size), :]\n",
    "    batch_labels2 =  np.asarray(train_label2)[offset:(offset + batch_size), :]\n",
    "    batch_labels3 =  np.asarray(train_label3)[offset:(offset + batch_size), :]\n",
    "    batch_labels4 =  np.asarray(train_label4)[offset:(offset + batch_size), :]\n",
    "    batch_labels5 =  np.asarray(train_label5)[offset:(offset + batch_size), :]    \n",
    "   # print(batch_labels1.shape)\n",
    "    feed_dict = {tf_train_dataset : batch_data\n",
    "              #  , tf_valid_dataset : batch_data_valid\n",
    "                , tf_train_labels0 : batch_labels0\n",
    "                , tf_train_labels1 : batch_labels1\n",
    "                , tf_train_labels2 : batch_labels2\n",
    "                , tf_train_labels3 : batch_labels3\n",
    "                , tf_train_labels4 : batch_labels4\n",
    "                , tf_train_labels5 : batch_labels5\n",
    "                }\n",
    "\n",
    "    _, l, pred_train0, pred_train1, pred_train2, pred_train3, pred_train4, pred_train5  \\\n",
    "        , pred_valid0, pred_valid1, pred_valid2, pred_valid3, pred_valid4, pred_valid5  \\\n",
    "        = session.run([optimizer, loss, train_prediction0, train_prediction1, train_prediction2  \\\n",
    "                       , train_prediction3, train_prediction4, train_prediction5, valid_prediction0  \\\n",
    "                       , valid_prediction1, valid_prediction2, valid_prediction3 , valid_prediction4, valid_prediction5]  \\\n",
    "                      , feed_dict=feed_dict)\n",
    "     \n",
    "    #normalize data to mean 0 and unit variance\n",
    "  #  batch_data = (batch_data - np.mean(batch_data))/np.std(batch_data)\n",
    "    #batch_data = np.reshape(batch_data,(batch_size,image_size*image_size))\n",
    "   # print(batch_data.shape)\n",
    "   # batch_valid_labels0 =  np.asarray(valid_label0)[offset:(offset + batch_size), :]\n",
    "   # batch_valid_labels1 =  np.asarray(valid_label1)[offset:(offset + batch_size), :]\n",
    "   # batch_valid_labels2 =  np.asarray(valid_label2)[offset:(offset + batch_size), :]\n",
    "   # batch_valid_labels3 =  np.asarray(valid_label3)[offset:(offset + batch_size), :]\n",
    "   # batch_valid_labels4 =  np.asarray(valid_label4)[offset:(offset + batch_size), :]\n",
    "   # batch_valid_labels5 =  np.asarray(valid_label5)[offset:(offset + batch_size), :]  \n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch training loss at step %d: %f' % (step, l))\n",
    "      print(\"Training accuracy - digit count: {:.1f}%; digits 1-5: {:.1f}%, {:.1f}%, {:.1f}%, {:.1f}%, {:.1f}%\".format( \\\n",
    "           accuracy(pred_train0, batch_labels0),accuracy(pred_train1, batch_labels1)  \\\n",
    "        ,accuracy(pred_train2, batch_labels2), accuracy(pred_train3, batch_labels3)  \\\n",
    "        ,accuracy(pred_train4, batch_labels4), accuracy(pred_train5, batch_labels5)))\n",
    "        \n",
    "      avg_acc_train = (accuracy(pred_train0, batch_labels0) + accuracy(pred_train1, batch_labels1)\n",
    "        + accuracy(pred_train2, batch_labels2) + accuracy(pred_train3, batch_labels3)\n",
    "        + accuracy(pred_train4, batch_labels4) + accuracy(pred_train5, batch_labels5))/6\n",
    "      print('Training avg accuracy: %.1f%%' % avg_acc_train)\n",
    "        \n",
    "      print(\"Validation accuracy - digit count: {:.1f}%; digits 1-5: {:.1f}%, {:.1f}%, {:.1f}%, {:.1f}%, {:.1f}%\".format( \\\n",
    "           accuracy(pred_valid0, valid_label0[:batch_size]),accuracy(pred_valid1, valid_label1[:batch_size])  \\\n",
    "        ,accuracy(pred_valid2, valid_label2[:batch_size]), accuracy(pred_valid3, valid_label3[:batch_size])  \\\n",
    "        ,accuracy(pred_valid4, valid_label4[:batch_size]), accuracy(pred_valid5, valid_label5[:batch_size])))\n",
    "    \n",
    "      avg_acc_valid = (accuracy(pred_valid0, valid_label0[:batch_size]) + accuracy(pred_valid1, valid_label1[:batch_size])\n",
    "        + accuracy(pred_valid2, valid_label2[:batch_size]) + accuracy(pred_valid3, valid_label3[:batch_size])\n",
    "        + accuracy(pred_valid4, valid_label4[:batch_size]) + accuracy(pred_valid5, valid_label5[:batch_size]))/6\n",
    "      print('Validation avg accuracy: %.1f%%' % avg_acc_valid)\n",
    "    \n",
    "    if avg_acc_valid > max_acc:\n",
    "        max_acc = avg_acc_valid\n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(session, \"C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\")\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    # print(\"w1 mean: {} std: {}\".format(np.mean(w), np.std(w)))\n",
    "     # print(\"wy1 mean: {} std: {}\".format(np.mean(w1), np.std(w1)))\n",
    "      #print(\"label: \", batch_labels1)\n",
    "    #  print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), L_valid))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Time: %0.2fs\" % (t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Restore model to predict on test data\n",
    "## Create graph for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=1\n",
    "#graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    #with tf.device('/gpu:0'):\n",
    "    \n",
    "  def weight_variable(w_id, shape, stdev_num):\n",
    "    #return (tf.Variable(tf.truncated_normal(shape,stddev=np.sqrt(2.0/(stdev_num)))))\n",
    "    return (tf.get_variable(w_id,  initializer = tf.truncated_normal(shape,stddev=np.sqrt(2.0/(stdev_num)))))\n",
    "\n",
    "  def bias_variable(b_id, shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    #return tf.Variable(initial)\n",
    "    return tf.get_variable(b_id, initializer = initial)\n",
    "    \n",
    "  tf_test_dataset = tf.placeholder(tf.float32, shape=[1, image_size, image_size, 1]) \n",
    "\n",
    "  def conv_layers(layer_num, data, weights, biases, input_size,filter_size, num_filters, valid=False, use_pooling=True): \n",
    "   # ########\n",
    "   # # Conv with leaky relu and 2x2 max pooling\n",
    "   # ########\n",
    "\n",
    "    if not valid:\n",
    "    # create filter-weights and bias during training\n",
    "        shape = [filter_size, filter_size, input_size, num_filters]\n",
    "        stdev_n = (filter_size*filter_size*input_size)\n",
    "        weights.append(weight_variable(\"cvw0_\"+layer_num, shape, stdev_n))\n",
    "        biases.append(bias_variable(\"cvb0_\"+layer_num, [num_filters]))\n",
    "\n",
    "    hidden=[]\n",
    "    y0 =  tf.nn.conv2d(input=data,filter=weights[0],strides=[1, 1, 1, 1],padding='SAME')\n",
    "    hidden.append(tf.maximum((y0 + biases[0]), 0.01*(y0+biases[0])))\n",
    "    \n",
    "    for L in range(1,num_conv_layers+1):   \n",
    "        if not valid:\n",
    "            shape = [filter_size, filter_size, num_filters, num_filters]\n",
    "            stdev_n = filter_size*filter_size*num_filters\n",
    "                \n",
    "            weights.append(weight_variable(\"cvw\"+str(L)+\"_\"+layer_num, shape, stdev_n))\n",
    "            biases.append(bias_variable(\"cvb\"+str(L)+\"_\"+layer_num, [num_filters]))   \n",
    "\n",
    "        y = tf.nn.conv2d(input=hidden[L-1],filter=weights[L],strides=[1, 1, 1, 1],padding='SAME')\n",
    "        hidden.append(tf.maximum((y + biases[L]), 0.01*(y+biases[L])))\n",
    "\n",
    "    layer = tf.nn.max_pool(value=hidden[num_conv_layers], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    return layer, weights, biases\n",
    "    \n",
    "  def fc_layers(layer_num, weights, biases, out_weights, out_biases, data, label_count, valid = False):\n",
    "   #  #####\n",
    "   #  # FC Layers - with leaky relu and dropout\n",
    "   #  #####\n",
    "\n",
    "    out_size = data.get_shape().as_list()[1]\n",
    "    if not valid:\n",
    "        #create weights and biases during training\n",
    "        weights.append(weight_variable(\"fcw0_\"+layer_num, [out_size, fc_size], fc_size*fc_size)) # w = 1024 depth * 1024 FC hidden node layer\n",
    "        biases.append(bias_variable(\"fcb0_\"+layer_num, [fc_size])) #  # b = 1024 FC hidden node layers1024\n",
    "\n",
    "    hidden=[]\n",
    "    y0 = (tf.matmul(data, weights[0]) + biases[0])\n",
    "    hidden.append(tf.nn.dropout(tf.maximum(y0, 0.01*y0), drop_rate1))\n",
    "    \n",
    "    for L in range(1,num_fc_layers+1):   \n",
    "        if not valid:\n",
    "            weights.append(weight_variable(\"fcw\"+str(L)+\"_\"+layer_num,[fc_size, fc_size], fc_size*fc_size))\n",
    "            biases.append(bias_variable(\"fcb\"+str(L)+\"_\"+layer_num,[fc_size]))   \n",
    "        y = ((tf.matmul(hidden[L-1], weights[L]) + biases[L])) \n",
    "        hidden.append(tf.nn.dropout(tf.maximum(y, 0.01*y), drop_rate2))\n",
    "        \n",
    "      #Definitive output  after relu layer's:  \n",
    "    if not valid:\n",
    "        out_weights = weight_variable(\"rlw_\"+layer_num,[fc_size, label_count], fc_size*label_count)\n",
    "        out_biases = bias_variable(\"rlb_\"+layer_num,[label_count])      \n",
    "    logits = tf.matmul(hidden[num_fc_layers], out_weights) + out_biases\n",
    "        \n",
    "    return logits, weights, biases, out_weights, out_biases\n",
    "    \n",
    "  \n",
    "  # Test data computation.\n",
    "  convpool1v, _, _ = conv_layers(\"1\", tf_test_dataset, wc1, bc1, num_channels, filter_size1, num_filters1, True)\n",
    "  convpool2v, _, _ = conv_layers(\"2\", convpool1v, wc2, bc2, num_filters1, filter_size2, num_filters2, True)\n",
    "  convpool3v, _, _ = conv_layers(\"3\", convpool2v, wc3, bc3, num_filters2, filter_size3, num_filters3, True)\n",
    "  shapev = convpool3v.get_shape().as_list()\n",
    "  conv_layers_reshape_valid = tf.reshape(convpool3v, [shapev[0], shapev[1] * shapev[2] * shapev[3]]) #[256, 4, 4, 64]\n",
    "\n",
    "  logits0_test, _, _, _, _ = fc_layers(\"1\", w0, b0, out_w0, out_b0, conv_layers_reshape_valid, num_digits, True) \n",
    "  logits1_test, _, _, _, _ = fc_layers(\"2\", w1, b1, out_w1, out_b1, conv_layers_reshape_valid, num_labels, True) \n",
    "  logits2_test, _, _, _, _ = fc_layers(\"3\", w2, b2, out_w2, out_b2, conv_layers_reshape_valid, num_labels, True) \n",
    "  logits3_test, _, _, _, _ = fc_layers(\"4\", w3, b3, out_w3, out_b3, conv_layers_reshape_valid, num_labels, True) \n",
    "  logits4_test, _, _, _, _ = fc_layers(\"5\", w4, b4, out_w4, out_b4, conv_layers_reshape_valid, num_labels, True) \n",
    "  logits5_test, _, _, _, _ = fc_layers(\"6\", w5, b5, out_w5, out_b5, conv_layers_reshape_valid, num_labels, True) \n",
    "\n",
    " # Predictions test data.\n",
    "  test_prediction0 = tf.nn.softmax(logits0_test)\n",
    "  test_prediction1 = tf.nn.softmax(logits1_test)\n",
    "  test_prediction2 = tf.nn.softmax(logits2_test)\n",
    "  test_prediction3 = tf.nn.softmax(logits3_test)\n",
    "  test_prediction4 = tf.nn.softmax(logits4_test)\n",
    "  test_prediction5 = tf.nn.softmax(logits5_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32, 1)\n",
      "INFO:tensorflow:Restoring parameters from C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\n",
      "Model restored.\n",
      "[5, 0, 10, 10, 10]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "test_data = valid_dataset[[2]]\n",
    "print(test_data.shape)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  saver.restore(session, \"C:/Users/rphila/Documents/Online Courses/DeepLearning/model/model.ckpt\")\n",
    "  print(\"Model restored.\")\n",
    "\n",
    "  feed_dict = {tf_test_dataset : test_data}\n",
    "                 \n",
    "  pred0, pred1, pred2, pred3, pred4, pred5  \\\n",
    "        = session.run([test_prediction0, test_prediction1, test_prediction2, test_prediction3\n",
    "                       , test_prediction4, test_prediction5] , feed_dict=feed_dict)\n",
    "\n",
    "  pred = []\n",
    "  pred.append(pred1[0].tolist().index(max(pred1[0])))\n",
    "  pred.append(pred2[0].tolist().index(max(pred2[0])))\n",
    "  pred.append(pred3[0].tolist().index(max(pred3[0])))\n",
    "  pred.append(pred4[0].tolist().index(max(pred4[0])))\n",
    "  pred.append(pred5[0].tolist().index(max(pred5[0])))\n",
    "  print(pred)\n",
    "  predict_lbl =''\n",
    "  for i in range(0, pred0[0].tolist().index(max(pred0[0]))):\n",
    "    predict_lbl = predict_lbl + str(pred[i])\n",
    "    \n",
    "  print(predict_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-d3c6109714cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredict_lbl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-419045509abe>\u001b[0m in \u001b[0;36mplot_images\u001b[1;34m(images, cls_true, cls_pred)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Plot image.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Show true and predicted classes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHi1JREFUeJzt3X2MHOW15/HvoY1NGMAkxgQy9tpGODhOwmbNCFBWQllF\nkYhXsu+K6Mr8kziCtVDCjbTSSstVVpuIPzab3KuNgoIWWY51Sf4Ab9Bd7SCRRb43kSKtEsN4RQDH\nwhmcOB6/BL9hbGPmxXP2j67TXVPT42mm36b6+X2kUb9UddXTPtbpU0899ZS5OyIi0v+u6XUDRESk\nO5TwRUQSoYQvIpIIJXwRkUQo4YuIJEIJX0QkEUr4gpntNrN3zOzNOZabmT1lZqNm9rqZbep2G6U1\nirGAEr5U/QPw4FWWfxlYn/3tAP5HF9ok7fUPKMbJU8IX3P3XwNmrrLIV+KlX/Ra42cxu707rpB0U\nYwFY0usGLNQ111zjlUpl1vtmBkAsq1QqLFmyJD4z43EucfXx1NQUAJOTk7X3p6enAWY9Fj9b3Fe+\nPfnH/GdiW1euXJnxOpZPTk6edveVV218ZwwCR3Ovx7L3ThRXNLMdVCtEBgYG7tmwYUNXGihV+/fv\nX+j/EcW4JFqIcXkTfqVS4aMf/WjtdSTWa6+9FoDly5cDcPPNN9fWu/766wH4yEc+ctVtT0xMAHD2\nbLUgeueddwC4fPkyH3zwQe15/jGSdDxed911ANxwww2z2gNw00031fYX23z//fcBOH/+fMNtj42N\nHblqwxcBd98J7AQYGhrykZGRHrcoLWbW8f8jinFvtRLj0iZ8mFmpx/OlS5cC9US7YsUKPvWpTwH1\nJBvL4jPFaj0S7/Hjx4H6D8WJEydqPwLF6j+2FevGYyT4+NG59dZbAbjtttuAajK/ePEiAOfOnQPq\nRykhfhB66BiwOvd6Vfae9A/FOAHqw5dmDANfzUZy3A+cd/dZh/pSaopxAkpb4ZsZ11xzTa0qjz7x\ngYEBoF5ZDw4O8vnPfx6Aj33sY0C9e2XZsmUztjk+Pg7UK+0//vGPALz11ltAtZq/dOkSUK/wi/3t\nsf/YR1Tyxce77rqrts3YXxxRxDmH8O677zb5r7IwZvYc8AXgFjMbA74DXAvg7s8ALwGbgVHgfeDr\nHW2QtJ1iLFDihC/t4+4Pz7PcgW92qTnSAYqxQIkT/pUrV2rVNsweLRMV9rp162p993FCd8WKFTMe\no9KPbUSlH/3uFy5cAOqVfn7dqPSjKl+5snry/OMf/zhQPcIA+MQnPtHwccWKFbXzAGNjYwAcOHBg\nxjbj5K2ISCvUhy8ikojSVvjuzsTERK3vPvrSQ1TtN9xwQ62CjtE3UemHGLUTn8l/FupDLPN968U+\n/DgaiAo/+urjXELsMz4XI3NuvfXW2tFIiHZG3/2pU6eu/o8hItIEVfgiIokobYUfosKPCjrGw0cl\nffny5VqFHBdQHTp0CKhX8DE2Pkbx3HLLLTPWj2o8P+4/+vlDVParV1eHMsf5gTi6OHr06IzHWD4+\nPs6qVatmfIdoz/r164HOj9IRkTSUNuGbGZVKZdYUBvEY3S+Tk5OcPHkSgCNHqheoxZWskWCLyToS\nbqx35swZoNrVEgk8unKi+6d4Iji2HZ+NRP/ee+8B9at9L1++XPvxiP3H0M444RvDNUVEWqEuHRGR\nRJS2wp9LdPHku1+iUo/H6KoJUbWHqMJj/egSunDhQm2agxiWGZV6VPb5yh3qF3EdO3ZsxrbyiieJ\n16xZA9S7mGKIp4hIK1Thi4gkovQV/lzTD0fVPDAwUHselXxU0tHPHq+jOi/2u0eVfuHChdrJ4Njv\njTfeCDBraGUcJcS24iKxaEO8f/3119eGdEYlH48xAVtx2yIiC6EKX0QkEaWu8BvdACVG58TFUsuX\nL581NXFU2VHhR2Uf/fIxhcLbb78N1IdFXrx4cVZ/fxwdxPQN0aa4eKo4fLN40deFCxdq1X5MvRz9\n/1HZq8IXkXZQhS8ikohSV/h5c/XhL1++vHYDlOIkZFHZF0fWRFUeF2jFHbAmJiZmTccc/exR6cfy\n4nj9qOyLN2iZnp6u9ffHkUTsP26EEuuKiLRCFb6ISCJKW+HHDVCico5quDi1wuTkZK3ajjH0UXVH\nf3/0v0e1/ulPfxqoj52PPvbLly/PmqSteC/dYh9/8V63IX9f3WhXTMMc/f/RvrjyVkSkFarwRUQS\nUdoKfy5RLcfjuXPnanPRxHj6qMKjso8+85hTJx7jNoRxY5KTJ0/WrtKNij6q7zg6iKtzo98/jjTi\ndfEK3YmJidr+Y5K2eCyOJhIRaYUqfBGRRJS2wnf3Gbc1zE+HDDOvlo2qPEbBFK+4jVkqN27cCNSn\nJV63bt2M1wcPHpxV2cd8N1GxR4Ufffaxr2hr9dahM/vwo/ovVvhR+efnBRIRWShlEhGRRJS6wp+Y\nmKiNt4+KOir8mJVyYGCgVl0X+/CLc9bHNuK2hNEvHzdEufHGG2uVfVy1G7cyjKt5Y+79YmUf4++L\nc/Dk2xwVfbyO8xD5dTvFzB4EfgRUgF3u/t8Ky7cDfwccy976sbvv6njDpG0UYyltwpf2MbMK8DTw\nJWAMeNXMht3994VV97j7411voLRMMRYoecJ391pVHNV5XAEb89IsXbq0VmXHFa1ROYd4P+bf+exn\nPwvU++djNM+yZctqFX5U/TGipziXTiiO049KP/Y1Pj5eOxqIx/hOxRuld9C9wKi7HwYws+eBrUAx\nGUh5KcZS7oRvZrWLlIpTK0S3yMmTJ2vdLXGitNj9Eyda46RunESN7qJIznnFKRWKE7HF+/FYbGec\nmJ2enq4l9vhhKib44gRsHTAIHM29HgPua7DeQ2b2AHAI+A/ufrTBOrI4Kcaik7bStBeBte5+N7AX\neLbRSma2w8xGzGyk0d29ZFFTjPtcqSv8GOLYSHSlXHfddXNeuBRdKMXpGKLLp9ilsmTJktp0B7Es\nKvfYRtzqMLp84iRurFfc5/T09KzKvjgBW374aYccA1bnXq+ifuIOAHc/k3u5C/hBow25+05gJ8DQ\n0NDcAZJuU4xFFb4A8Cqw3szWmdlSYBswnF/BzG7PvdwCHOxi+6R1irGUt8J39xl93TF5WlTg0Xe+\ncuXKWjUdfeFROUeVnr8IKv9+VOXR179s2bJav368FyeHowqP4Zpxm8K4iUpsM9oQ28lX78WLtbrQ\ndw+Au0+Z2ePAy1SH7O129wNm9iQw4u7DwLfMbAswBZwFtnelcdIWirFAiRO+tJe7vwS8VHjvv+Se\n/y3wt91ul7SPYiylT/hRKcfolxj2GBcrxbQJAH/5y1+A+oiaOBooTl0Qo3ZiSoa4YOvKlSu17Udl\nHkcBUZ3H8Mx4jBE+sa8Q28wP4ywO6Yxtd6EPX0QSoD58EZFElL7Cj2o9HqPij7H3a9asmXW7weh3\nj9dxNBBVeHECtqj4P/jgg9pnilMax9j96MMvjv2P6j3WizZVKpVZN0KP71A8lyAi0gpV+CIiiSht\nhW9mVCqVWVfDFqc+WLNmzazROadPn65tA+oVflw9e7XpiqPajr78mJYh3o9txHj85cuXA7Ov7o3K\n/8Ybb6y1dXBwsOFnor9fRKQVqvBFRBJR2gofqhV69N1HH3lU2FHp33TTTbV1iuPcozovVuWxrZhb\nJyr8S5cucenSJaBedccRxtGj1SlHNm3aBFSPLKB+85TDhw8D9Qo/Rt7kJ2SLdsaVvnHZ+okTJz70\nv42ISJEqfBGRRJS6ws+LETbFseyVSqU2cqao2B8f24g+/+jrj9E6Fy9e5MKFC0C9Uo/PRoUfRwNx\ntBC3TYwbocfRRax3/fXX184hRIUfy2L/qvBFpB1U4YuIJKL0FX5UzFGtx5j5mKXyyJEjtdEvMTY+\n+s+Lc9UUPxtV+ZEjR4Bqn358Nh6j4v/Tn/4EwOjoKFA/LxA3Qt+8eTMAn/zkJwF44403atuJo5I4\nwojzBHGlbxw9iIi0QhW+iEgiSl/hF28AHtVy9HsfPHiQVatWAfU+8jgaKI7Pj1ExUdFHZR2VfvSt\nw+y5cWJ/+/btA+pHAGvXrgXqo3bitolxde27775ba0+MGoojjZj7R334ItIOqvBFRBJR6grf3Wt9\n6MXROcePHweqVX3xRuMxR01U9nGUEH3mhw4dAuqjc2K0TGwP6jNsxiid6Pf/zW9+M2ObUekPDQ0B\n9XH5caXt0aNHa0cSxSOPuA4g5v4REWlFqRM+1G9zWLw9YCTN48ePs3fvXgBuu+22GZ8t3kowEmz8\nWMQUC5HcK5XKrP3E6+juOXiwepOg4i0N40cj2hDtO3LkCH/+85+BetdN/ABEF1O+K0lEZKHUpSMi\nkohSV/hmVpsArdilE9MTXLlypXYSNKZDKN7wpHhUEOuHWH9ycnJWZR/muy1hdA9Ft1J0DZ08ebK2\nLNoXRwNR2Rf3JSKyEKrwRUQSUdoK38xYunRpbVrkoqi4p6enaxOcRSVdFEcHUUkXb3oexsfHa+sU\nb3WYnwwt/9mYNC2GdubbD9VzEHGuoHhOobjNOGoREVkIVfgiIokobYU/n/yNv6MqL1byxeWNPtvo\ndaPPzLe8eCSQV6zo47PxGCORRERaoQpfADCzB83sLTMbNbMnGixfZmZ7suX7zGxt91sprVCMJemE\nPz09PeNvvvXyrly5MuPvaus2en9ycrL2F8smJiaYmJhgamqKqakp3B13n7WPdjOzCvA08GVgI/Cw\nmW0srPYIcM7d7wR+CHy/Yw2StlOMBRJP+FJzLzDq7ofdfQJ4HthaWGcr8Gz2/AXgixZnnqUMFGMp\nbx/+9PT06ffee+9Ir9vRZWs6tN1BID8H8xhw31zruPuUmZ0HVgCn8yuZ2Q5gR/Zy3Mze7EiLO+sW\nCt+rRO6a433FeKZ+jPG8Spvw3X1lr9sgs7n7TmAngJmNuPtQj5v0oZW13VBte6f3oRj3VisxVpeO\nABwDVuder8rea7iOmS0BlgONL2yQxUgxFiV8AeBVYL2ZrTOzpcA2YLiwzjDwtez5V4BfusaLloli\nLOXt0pH2yfprHwdeBirAbnc/YGZPAiPuPgz8BPiZmY0CZ6kmjPns7FijO6us7YY52q4Yz1LWdkML\nbbd+/wE3sxXAP2cvbwOuAKey1/dmIxY6sd8x4Fy2v3F3vy/Xnv8J/AvgMPDX7n5+zg2JiLRJ3yf8\nPDP7LnDR3f++8L5R/beYezD+h9/XGPAZd3+38P5/B467+9+b2X8GPuLu327XfkVE5pJsH76Z3Wlm\nb5rZM8D/A1ab2bu55dvMbFf2/ONm9o9mNmJmr5jZ/S3sOj/W+Vngr1rYlohI05JN+JmNwC53/1fM\nHrGQ9xTwg2wY118D8UNwX/aD0YgDvzSz/Wb2SO79Fe4eXUrHgNtb+gaLQFkv2W+i3dvN7JSZvZb9\nPdqLdhaZ2W4ze2eu8e9W9VT2vV43s01t2Kdi3EUdi3Fcvp/CH/Bd4D9mz+8E/pBbtgR4N/d6G9Uf\nA6gOTXst93eMalfM1fY1mD3eBrwBfD57nd/HNcDZXv+7tPhvWgHeBu4AlgK/AzYW1vkG8Ezu33VP\nSdq9Hfhxr9vaoO0PAJuAN+dYvhn4BWDA/cA+xVgxdvfkK/xLuefTVP/xQn6ifaN6gvdz2d+gu1++\n2obd/Vj2eBL431QvbQc4Y2Zx0dggcKKVL7AIlPWS/WbavSi5+6+pjqKZy1bgp171W+BmM2vlSFIx\n7rJOxTj1hF/j1RO258xsvZldA/y73OJ/Ar4ZL8zsc1fblpndYGY3ZM8HgC8BcWiWH+v8Nao/BmXW\n6JL9wbnWcfcpIC7Z76Vm2g3wUHbI/IKZrW6wfDFq9ru1c3uKcXctKMZK+DP9J+D/UB3Gmb9F1TeB\nf539p/g98O/hqn34twP/18x+B7wC/C93/6ds2X8F/q2Z/YHqYdvfdearSBu8CKx197uBvdQrWOkf\nScU4qQuv3P27ueejwOcKy/cAexp87hTVKw+L7+8D9jV4/w/Av5yjDaeAf/Mhm76YfZhL9scW0SX7\n87bb3fNt3AX8oAvtaodmYtLu7SnG3bWgGKvCl1aV9ZL9edtd6BPdAhzsYvtaMQx8NRvJcT9w3t1b\nOVekGC8+C4txE2eLdwPvMPfZYqM6bHEUeB3Y1Osz3Pr7cH+txpjqiIFDVEdEfDt770lgS/b8OuDn\n2edfAe7o9Xdust3fAw5QHd3xK2BDr9uctes5qif7J6l2PT4CPAY8lovX09n3egMYUoz7P8bNbHfe\nK23N7AHgItUzwp9psHwz8DfZP+x9wI88m0ZAykEx7n+KsUATXTre/SFg0mWKcf9TjAXac9J2ruFB\ns/qTLHennIGBgXs2bNjQht1Ls/bv33/aF3bjGMW4JBTj/tdCjLs7Ssdzd8oZGhrykZGO35xHcsys\n47eEVIx7SzHuf63EuB2jdNo9BEwWH8W4/ynGCWhHwm/3EDBZfBTj/qcYJ2DeLh0zew74AnCLVed4\n/w5wLYC7PwO8RPXM/ijwPvD1TjVWOkMx7n+KsUATCd/dH55nuZObZ0bKRzHuf4qxgK60FRFJhhK+\niEgilPBFRBKhhC8ikgglfBGRRCjhi4gkQglfRCQRSvgiIolQwhcRSYQSvohIIpTwRUQSoYQvIpII\nJXwRkUQo4YuIJEIJX0QkEUr4IiKJUMIXEUmEEr6ISCKU8EVEEqGELyKSCCV8EZFENJXwzexBM3vL\nzEbN7IkGy7eb2Skzey37e7T9TZVOUoz7n2IsS+ZbwcwqwNPAl4Ax4FUzG3b33xdW3ePuj3egjdJh\ninH/U4wFmqvw7wVG3f2wu08AzwNbO9ss6TLFuP8pxtJUwh8EjuZej2XvFT1kZq+b2QtmtrrRhsxs\nh5mNmNnIqVOnFtBc6RDFuP8pxtK2k7YvAmvd/W5gL/Bso5Xcfae7D7n70MqVK9u0a+kSxbj/KcZ9\nrpmEfwzI/9Kvyt6rcfcz7j6evdwF3NOe5kmXKMb9TzGWphL+q8B6M1tnZkuBbcBwfgUzuz33cgtw\nsH1NlC5QjPufYizzj9Jx9ykzexx4GagAu939gJk9CYy4+zDwLTPbAkwBZ4HtHWyztJli3P8UYwEw\nd+/JjoeGhnxkZKQn+06Vme1396Fu7U8x7j7FuP+1EmNdaSsikgglfBGRRCjhi4gkQglfRCQRSvgi\nIolQwhcRSYQSvohIIpTwRUQSoYQvIpIIJXwRkUQo4YuIJEIJX0QkEUr4IiKJUMIXEUmEEr6ISCKU\n8EVEEqGELyKSCCV8EZFEKOGLiCRCCV9EJBFK+CIiiWgq4ZvZg2b2lpmNmtkTDZYvM7M92fJ9Zra2\n3Q2VzlKM+59iLPMmfDOrAE8DXwY2Ag+b2cbCao8A59z9TuCHwPfb3VDpHMW4/ynGAs1V+PcCo+5+\n2N0ngOeBrYV1tgLPZs9fAL5oZta+ZkqHKcb9TzEWljSxziBwNPd6DLhvrnXcfcrMzgMrgNP5lcxs\nB7AjezluZm8upNGLwC0UvltJ3DXH+4rxTGWNLyjGzerHGM+rmYTfNu6+E9gJYGYj7j7Uzf23S1nb\nbmYjnd5HP8S4rO0GxbhZZW03tBbjZrp0jgGrc69XZe81XMfMlgDLgTMLbZR0nWLc/xRjaSrhvwqs\nN7N1ZrYU2AYMF9YZBr6WPf8K8Et39/Y1UzpMMe5/irHM36WT9eU9DrwMVIDd7n7AzJ4ERtx9GPgJ\n8DMzGwXOUv3PNJ+dLbS718ra9obtVoxnKWu7QTFuVlnbDS203fQDLiKSBl1pKyKSCCV8EZFEdDzh\nl/Vy7ibavd3MTpnZa9nfo71oZ5GZ7Tazd+YaG21VT2Xf63Uz29SGfSrGXaQYN08xLnD3jv1RPTn0\nNnAHsBT4HbCxsM43gGey59uAPZ1sUxvbvR34ca/b2qDtDwCbgDfnWL4Z+AVgwP3APsVYMVaMex/X\nbsS40xV+WS/nbqbdi5K7/5rqCIu5bAV+6lW/BW42s9tb2KVi3GWKcdMU44JOJ/xGl3MPzrWOu08B\ncTl3LzXTboCHssOpF8xsdYPli1Gz362d21OMu0sxrlKMC3TSduFeBNa6+93AXurVjfQPxbj/JRXj\nTif8sl7OPW+73f2Mu49nL3cB93Spba1qJibt3p5i3F2KcZViXNDphF/Wy7nnbXehv2wLcLCL7WvF\nMPDV7Cz//cB5dz/RwvYU48VHMa5SjIuaOFu8G3iHuc8WG/AUMAq8DmxqcDb5ENWz5d/O3nsS2JI9\nvw74efb5V4A7en2GvMl2fw84QPXM/6+ADb1uc9au54ATwCTVfr1HgMeAx3Lxejr7Xm8AQ4qxYqwY\nlz/GzWx33qkVzOwB4CLVM8KfabB8M/A32T/sfcCP3L04z7YsYopx/1OMBZro0vHuDwGTLlOM+59i\nLNCeG6DMNTxoVn+S5e6UMzAwcM+GDRvasHtp1v79+0+7+8oFfFQxLgnFuP+1EOPe3fFqaGjIR0Y6\nfnMeyTGzI53eh2LcW4px/2slxu0YpdPuIWCy+CjG/U8xTkA7En67h4DJ4qMY9z/FOAHzdumY2XPA\nF4BbzGwM+A5wLYC7PwO8RPXM/ijwPvD1TjVWOkMx7n+KsUBztzh8eJ7lDnyzbS2SrlOM+59iLKC5\ndEREkqGELyKSCCV8EZFEKOGLiCRCCV9EJBFK+CIiiVDCFxFJhBK+iEgilPBFRBKhhC8ikgglfBGR\nRCjhi4gkQglfRCQRSvgiIolQwhcRSYQSvohIIpTwRUQSoYQvIpIIJXwRkUQo4YuIJEIJX0QkEU0l\nfDN70MzeMrNRM3uiwfLtZnbKzF7L/h5tf1OlkxTj/qcYy5L5VjCzCvA08CVgDHjVzIbd/feFVfe4\n++MdaKN0mGLc/xRjgeYq/HuBUXc/7O4TwPPA1s42S7pMMe5/irE0lfAHgaO512PZe0UPmdnrZvaC\nma1uS+ukWxTj/qcYS9tO2r4IrHX3u4G9wLONVjKzHWY2YmYjp06datOupUsU4/6nGPe5ZhL+MSD/\nS78qe6/G3c+4+3j2chdwT6MNuftOdx9y96GVK1cupL3SGYpx/1OMpamE/yqw3szWmdlSYBswnF/B\nzG7PvdwCHGxfE6ULFOP+pxjL/KN03H3KzB4HXgYqwG53P2BmTwIj7j4MfMvMtgBTwFlgewfbLG2m\nGPc/xVgAzN17suOhoSEfGRnpyb5TZWb73X2oW/tTjLtPMe5/rcRYV9qKiCRCCV9EJBFK+CIiiVDC\nFxFJhBK+iEgilPBFRBKhhC8ikgglfBGRRCjhi4gkQglfRCQRSvgiIolQwhcRSYQSvohIIpTwRUQS\noYQvIpIIJXwRkUQo4YuIJEIJX0QkEUr4IiKJUMIXEUmEEr6ISCKU8EVEEtFUwjezB83sLTMbNbMn\nGixfZmZ7suX7zGxtuxsqnaUY9z/FWOZN+GZWAZ4GvgxsBB42s42F1R4Bzrn7ncAPge+3u6HSOYpx\n/1OMBZqr8O8FRt39sLtPAM8DWwvrbAWezZ6/AHzRzKx9zZQOU4z7n2IsLGlinUHgaO71GHDfXOu4\n+5SZnQdWAKfzK5nZDmBH9nLczN5cSKMXgVsofLeSuGuO9xXjmcoaX1CMm9WPMZ5XMwm/bdx9J7AT\nwMxG3H2om/tvl7K23cxGOr2PfohxWdsNinGzytpuaC3GzXTpHANW516vyt5ruI6ZLQGWA2cW2ijp\nOsW4/ynG0lTCfxVYb2brzGwpsA0YLqwzDHwte/4V4Jfu7u1rpnSYYtz/FGOZv0sn68t7HHgZqAC7\n3f2AmT0JjLj7MPAT4GdmNgqcpfqfaT47W2h3r5W17Q3brRjPUtZ2g2LcrLK2G1pou+kHXEQkDbrS\nVkQkEUr4IiKJ6HjCL+vl3E20e7uZnTKz17K/R3vRziIz221m78w1Ntqqnsq+1+tmtqkN+1SMu0gx\nbp5iXODuHfujenLobeAOYCnwO2BjYZ1vAM9kz7cBezrZpja2ezvw4163tUHbHwA2AW/OsXwz8AvA\ngPuBfYqxYqwY9z6u3Yhxpyv8sl7O3Uy7FyV3/zXVERZz2Qr81Kt+C9xsZre3sEvFuMsU46YpxgWd\nTviNLucenGsdd58C4nLuXmqm3QAPZYdTL5jZ6gbLF6Nmv1s7t6cYd5diXKUYF+ik7cK9CKx197uB\nvdSrG+kfinH/SyrGnU74Zb2ce952u/sZdx/PXu4C7ulS21rVTEzavT3FuLsU4yrFuKDTCb+sl3PP\n2+5Cf9kW4GAX29eKYeCr2Vn++4Hz7n6ihe0pxouPYlylGBd14WzzZuAQ1bPl387eexLYkj2/Dvg5\nMAq8AtzR6zPkTbb7e8ABqmf+fwVs6HWbs3Y9B5wAJqn26z0CPAY8li03qjfCeBt4AxhSjBVjxTiN\nGGtqBRGRROikrYhIIpTwRUQSoYQvIpIIJXwRkUQo4YuIJEIJX0QkEUr4IiKJ+P+87xNd4P3awAAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133ba484278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(images=test_data, cls_true=[predict_lbl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
